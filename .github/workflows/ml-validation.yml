name: ML Model Validation

on:
  push:
    branches: [main, develop]
    paths:
      - 'models/**'
      - 'data/processed/**'
      - 'src/ml_testing/**'
      - 'src/models/**'
  pull_request:
    branches: [main]
  schedule:
    # Run weekly on Sundays at midnight
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      full_validation:
        description: 'Run full validation including drift detection'
        required: false
        default: true
        type: boolean

jobs:
  # ============================================================================
  # ML TESTING - CORE VALIDATION
  # ============================================================================
  ml-validation:
    name: ML Model Validation
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true  # If using Git LFS for model files
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install deepchecks evidently pytest pytest-cov
      
      - name: Run ML validation tests
        id: ml_tests
        continue-on-error: true
        run: |
          python -m pytest tests/test_ml_validation.py -v --tb=short \
            --junitxml=ml_validation_results.xml \
            2>&1 | tee ml_validation_output.log
        env:
          PYTHONPATH: ${{ github.workspace }}
      
      - name: Upload ML validation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ml-validation-results
          path: |
            ml_validation_results.xml
            ml_validation_output.log
          retention-days: 30
      
      - name: Check ML validation status
        run: |
          if grep -q "FAILED" ml_validation_output.log; then
            echo "‚ö†Ô∏è Some ML validation tests failed"
            echo "Check the artifacts for detailed results"
            # Don't fail the workflow - just warn
          else
            echo "‚úÖ All ML validation tests passed"
          fi

  # ============================================================================
  # DATA QUALITY CHECK
  # ============================================================================
  data-quality:
    name: Data Quality Check
    runs-on: ubuntu-latest
    needs: ml-validation
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scipy scikit-learn
      
      - name: Run data quality validation
        id: data_quality
        run: |
          python << 'EOF'
          import sys
          import json
          from pathlib import Path
          
          # Check if data files exist
          data_dir = Path("data/processed/pppq")
          required_files = [
              "train/pppq_train.csv",
              "test/pppq_test.csv",
              "val/pppq_val.csv"
          ]
          
          results = {"status": "success", "checks": []}
          
          for file_path in required_files:
              full_path = data_dir / file_path
              if full_path.exists():
                  results["checks"].append({
                      "file": str(file_path),
                      "status": "found",
                      "exists": True
                  })
              else:
                  results["checks"].append({
                      "file": str(file_path),
                      "status": "missing",
                      "exists": False
                  })
          
          # Report results
          print("=== Data Quality Check Results ===")
          print(json.dumps(results, indent=2))
          
          # In CI, data files might not exist - that's OK
          missing_count = sum(1 for c in results["checks"] if not c["exists"])
          if missing_count > 0:
              print(f"\n‚ö†Ô∏è {missing_count} data files not found (expected in CI)")
          else:
              print("\n‚úÖ All data files found")
          
          sys.exit(0)  # Always pass - data might not be in repo
          EOF

  # ============================================================================
  # MODEL HEALTH CHECK
  # ============================================================================
  model-health:
    name: Model Health Check  
    runs-on: ubuntu-latest
    needs: ml-validation
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy lightgbm xgboost
      
      - name: Check model files
        id: model_check
        run: |
          python << 'EOF'
          import json
          from pathlib import Path
          
          model_dir = Path("models/pppq")
          
          results = {
              "lgbm_model": (model_dir / "lgbm_model.txt").exists(),
              "xgb_model": (model_dir / "xgb_model.json").exists(),
              "feature_columns": (model_dir / "feature_columns.json").exists()
          }
          
          print("=== Model Health Check ===")
          for model, exists in results.items():
              status = "‚úÖ" if exists else "‚ùå"
              print(f"{status} {model}: {'found' if exists else 'missing'}")
          
          # In CI, model files might not exist
          if all(results.values()):
              print("\n‚úÖ All model files present")
          else:
              print("\n‚ö†Ô∏è Some model files missing (expected in CI)")
          EOF

  # ============================================================================
  # VALIDATION REPORT
  # ============================================================================
  validation-report:
    name: Generate Validation Report
    runs-on: ubuntu-latest
    needs: [ml-validation, data-quality, model-health]
    if: always()
    
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: ml-validation-results
          path: ./results
        continue-on-error: true
      
      - name: Generate summary report
        run: |
          echo "# üî¨ ML Validation Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Pipeline Status" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ML Validation | ${{ needs.ml-validation.result == 'success' && '‚úÖ Passed' || '‚ö†Ô∏è Check Results' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Data Quality | ${{ needs.data-quality.result == 'success' && '‚úÖ Passed' || '‚ö†Ô∏è Check Results' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Model Health | ${{ needs.model-health.result == 'success' && '‚úÖ Passed' || '‚ö†Ô∏è Check Results' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Notes" >> $GITHUB_STEP_SUMMARY
          echo "- Data and model files may not be present in CI environment" >> $GITHUB_STEP_SUMMARY
          echo "- Full validation runs locally with actual data/models" >> $GITHUB_STEP_SUMMARY
          echo "- Scheduled runs occur weekly on Sundays" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Generated on: $(date -u)*" >> $GITHUB_STEP_SUMMARY
