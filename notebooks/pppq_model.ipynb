{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyBPOdFuONsR",
        "outputId": "a98d7ec2-9864-482c-e0c5-460858563411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "  PPP-Q ENHANCED CLASSIFIER - PRODUCTION TRAINING\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: LOADING & VALIDATING DATA\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Data loaded:\n",
            "   Train: (65745, 43)\n",
            "   Val:   (10950, 43)\n",
            "   Test:  (10695, 43)\n",
            "\n",
            "üìã Features: 38\n",
            "   Classes: ['A_PRESERVER', 'B_PARTIAL', 'C_ERODER', 'D_DESTROYER']\n",
            "\n",
            "================================================================================\n",
            "STEP 2: PREPARING FEATURES\n",
            "================================================================================\n",
            "\n",
            "üìä Feature columns: 38\n",
            "\n",
            "üè∑Ô∏è  Class Mapping:\n",
            "   0: A_PRESERVER\n",
            "   1: B_PARTIAL\n",
            "   2: C_ERODER\n",
            "   3: D_DESTROYER\n",
            "\n",
            "üìä Class Distribution:\n",
            "\n",
            "   TRAIN:\n",
            "   A_PRESERVER: 21,366 (32.5%)\n",
            "   B_PARTIAL: 20,283 (30.9%)\n",
            "   C_ERODER: 19,834 (30.2%)\n",
            "   D_DESTROYER: 4,262 (6.5%)\n",
            "\n",
            "   VAL:\n",
            "   A_PRESERVER: 3,942 (36.0%)\n",
            "   B_PARTIAL: 5,550 (50.7%)\n",
            "   C_ERODER: 1,323 (12.1%)\n",
            "   D_DESTROYER: 135 (1.2%)\n",
            "\n",
            "   TEST:\n",
            "   A_PRESERVER: 4,332 (40.5%)\n",
            "   B_PARTIAL: 3,999 (37.4%)\n",
            "   C_ERODER: 1,495 (14.0%)\n",
            "   D_DESTROYER: 869 (8.1%)\n",
            "\n",
            "================================================================================\n",
            "STEP 3: TRAINING PRIMARY MODEL - LIGHTGBM\n",
            "================================================================================\n",
            "\n",
            "üöÄ Training LightGBM...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttrain's multi_logloss: 0.0380366\tval's multi_logloss: 0.174263\n",
            "[100]\ttrain's multi_logloss: 0.0045951\tval's multi_logloss: 0.149023\n",
            "[150]\ttrain's multi_logloss: 0.00131698\tval's multi_logloss: 0.140677\n",
            "[200]\ttrain's multi_logloss: 0.000486233\tval's multi_logloss: 0.14516\n",
            "Early stopping, best iteration is:\n",
            "[189]\ttrain's multi_logloss: 0.000584089\tval's multi_logloss: 0.138944\n",
            "\n",
            "‚úÖ LightGBM trained!\n",
            "   Best iteration: 189\n",
            "   Training time: 30.55s\n",
            "\n",
            "================================================================================\n",
            "STEP 4: TRAINING SECONDARY MODEL - XGBOOST\n",
            "================================================================================\n",
            "\n",
            "üöÄ Training XGBoost...\n",
            "‚úÖ XGBoost trained!\n",
            "   Best iteration: 153\n",
            "   Training time: 12.50s\n",
            "\n",
            "================================================================================\n",
            "STEP 5: TRAINING BASELINE - RANDOM FOREST\n",
            "================================================================================\n",
            "\n",
            "üöÄ Training Random Forest...\n",
            "‚úÖ Random Forest trained!\n",
            "   Training time: 48.47s\n",
            "\n",
            "================================================================================\n",
            "STEP 6: EVALUATING ALL MODELS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "LIGHTGBM - TEST SET RESULTS\n",
            "================================================================================\n",
            "\n",
            "üìä Metrics:\n",
            "   Accuracy:          0.9442\n",
            "   Balanced Accuracy: 0.9480\n",
            "   Macro F1:          0.9543 ‚Üê PRIMARY METRIC\n",
            "   Weighted F1:       0.9446\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " A_PRESERVER       1.00      0.90      0.95      4332\n",
            "   B_PARTIAL       0.87      1.00      0.93      3999\n",
            "    C_ERODER       0.99      0.90      0.94      1495\n",
            " D_DESTROYER       1.00      0.99      0.99       869\n",
            "\n",
            "    accuracy                           0.94     10695\n",
            "   macro avg       0.96      0.95      0.95     10695\n",
            "weighted avg       0.95      0.94      0.94     10695\n",
            "\n",
            "\n",
            "================================================================================\n",
            "XGBOOST - TEST SET RESULTS\n",
            "================================================================================\n",
            "\n",
            "üìä Metrics:\n",
            "   Accuracy:          0.9402\n",
            "   Balanced Accuracy: 0.9404\n",
            "   Macro F1:          0.9487 ‚Üê PRIMARY METRIC\n",
            "   Weighted F1:       0.9406\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " A_PRESERVER       1.00      0.90      0.95      4332\n",
            "   B_PARTIAL       0.87      1.00      0.93      3999\n",
            "    C_ERODER       0.98      0.89      0.93      1495\n",
            " D_DESTROYER       1.00      0.98      0.99       869\n",
            "\n",
            "    accuracy                           0.94     10695\n",
            "   macro avg       0.96      0.94      0.95     10695\n",
            "weighted avg       0.95      0.94      0.94     10695\n",
            "\n",
            "\n",
            "================================================================================\n",
            "RANDOM FOREST - TEST SET RESULTS\n",
            "================================================================================\n",
            "\n",
            "üìä Metrics:\n",
            "   Accuracy:          0.9035\n",
            "   Balanced Accuracy: 0.8794\n",
            "   Macro F1:          0.8992 ‚Üê PRIMARY METRIC\n",
            "   Weighted F1:       0.9023\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " A_PRESERVER       1.00      0.89      0.94      4332\n",
            "   B_PARTIAL       0.80      1.00      0.89      3999\n",
            "    C_ERODER       0.98      0.65      0.78      1495\n",
            " D_DESTROYER       0.99      0.98      0.99       869\n",
            "\n",
            "    accuracy                           0.90     10695\n",
            "   macro avg       0.94      0.88      0.90     10695\n",
            "weighted avg       0.92      0.90      0.90     10695\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 7: CREATING ENSEMBLE MODEL\n",
            "================================================================================\n",
            "\n",
            "üìä ENSEMBLE (LightGBM + XGBoost) RESULTS:\n",
            "   Accuracy:    0.9433\n",
            "   Macro F1:    0.9531\n",
            "\n",
            "================================================================================\n",
            "STEP 8: FEATURE IMPORTANCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "üîù Top 20 Features:\n",
            "                    feature  importance_gain\n",
            "      PPP_Q_Composite_Score    993051.895240\n",
            "  Market_Cap_Saturation_Pct     93612.072832\n",
            "         Composite_Score_5Y     65281.583937\n",
            "        Vol_Adj_PP_Score_5Y     34712.410669\n",
            "             Volatility_90D     29801.629422\n",
            "Growth_Potential_Multiplier     28843.861916\n",
            "             Real_Return_5Y     22282.835696\n",
            "            Calmar_Ratio_5Y     21017.844842\n",
            "        Real_Return_Milk_1Y     15781.450331\n",
            "          Recovery_Strength      9443.127529\n",
            "             Days_Since_ATH      8317.123776\n",
            "         Return_Consistency      6280.048743\n",
            "        Real_Return_Eggs_1Y      6089.704675\n",
            "             Real_Return_3Y      5841.389001\n",
            "            Sharpe_Ratio_5Y      4344.946885\n",
            "               Max_Drawdown      3513.007433\n",
            "      Distance_From_ATH_Pct      3121.042945\n",
            "            CPI_Correlation      2974.651914\n",
            "              Real_PP_Index      2895.178999\n",
            "           PP_Multiplier_1Y      2882.960978\n",
            "\n",
            "================================================================================\n",
            "STEP 9: GENERATING ACTIONABLE INSIGHTS\n",
            "================================================================================\n",
            "\n",
            "üî® Generating insights for test set...\n",
            "‚úÖ Generated 10695 actionable insights\n",
            "\n",
            "üìã Sample Insights (Bitcoin):\n",
            "   asset: Bitcoin\n",
            "   predicted_class: B_PARTIAL\n",
            "   confidence: 100.0\n",
            "   volatility: HIGH\n",
            "   entry_signal: CONSIDER\n",
            "   cycle_position: VALUE_ZONE\n",
            "\n",
            "================================================================================\n",
            "STEP 10: SAVING MODELS & ARTIFACTS\n",
            "================================================================================\n",
            "\n",
            "üíæ All models and artifacts saved!\n",
            "\n",
            "================================================================================\n",
            "‚úÖ TRAINING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "üìä MODEL PERFORMANCE COMPARISON (Test Set):\n",
            "   Model                Macro F1     Accuracy    \n",
            "   --------------------------------------------\n",
            "   LightGBM             0.9543       0.9442      \n",
            "   XGBoost              0.9487       0.9402      \n",
            "   Random Forest        0.8992       0.9035      \n",
            "   Ensemble             0.9531       0.9433      \n",
            "\n",
            "üèÜ BEST MODEL: LightGBM\n",
            "\n",
            "üìÅ OUTPUT FILES:\n",
            "   Models: models/pppq/\n",
            "   Reports: reports/pppq/\n",
            "   Insights: reports/pppq/investment_insights.csv\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "PPP-Q ENHANCED CLASSIFIER - PRODUCTION TRAINING SYSTEM\n",
        "================================================================================\n",
        "Multi-Asset Investment Strategy Classifier with Cycle Awareness\n",
        "\n",
        "Models: LightGBM (Primary) + XGBoost (Ensemble) + Random Forest (Baseline)\n",
        "Output: A/B/C/D + Volatility + Cycle Position + Entry Signal + Actionable Insights\n",
        "\n",
        "Author: Bilal Ahmad Sheikh\n",
        "Institution: GIKI\n",
        "Date: December 2024\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML Libraries\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, f1_score,\n",
        "    accuracy_score, balanced_accuracy_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedTrainingConfig:\n",
        "    \"\"\"Enhanced configuration with asset-specific logic\"\"\"\n",
        "\n",
        "    # Paths\n",
        "    DATA_DIR = '/content/'\n",
        "    MODEL_DIR = 'models/pppq/'\n",
        "    REPORTS_DIR = 'reports/pppq/'\n",
        "\n",
        "    # Primary Model: LightGBM (Best for imbalanced data)\n",
        "    LGBM_PARAMS = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 4,\n",
        "        'metric': 'multi_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'max_depth': 7,\n",
        "        'min_child_samples': 20,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 0.1,\n",
        "        'verbose': -1,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'is_unbalance': True\n",
        "    }\n",
        "\n",
        "    # Secondary Model: XGBoost (Ensemble diversity)\n",
        "    XGB_PARAMS = {\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 4,\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'max_depth': 7,\n",
        "        'learning_rate': 0.05,\n",
        "        'n_estimators': 500,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 0.1,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'early_stopping_rounds': 50  # Add here\n",
        "    }\n",
        "\n",
        "    # Baseline: Random Forest (For comparison)\n",
        "    RF_PARAMS = {\n",
        "        'n_estimators': 300,\n",
        "        'max_depth': 15,\n",
        "        'min_samples_split': 10,\n",
        "        'min_samples_leaf': 5,\n",
        "        'class_weight': 'balanced',\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    # Training parameters\n",
        "    NUM_BOOST_ROUND = 500\n",
        "    EARLY_STOPPING_ROUNDS = 50\n",
        "    VERBOSE_EVAL = 50\n",
        "\n",
        "    # Asset categories\n",
        "    CRYPTO_ASSETS = ['Bitcoin', 'Ethereum', 'Litecoin']\n",
        "    PRECIOUS_METALS = ['Gold', 'Silver']\n",
        "    EQUITY_INDICES = ['SP500', 'NASDAQ', 'DowJones']\n",
        "    COMMODITIES = ['Oil']\n",
        "    ETFS = ['Gold_ETF', 'TreasuryBond_ETF', 'RealEstate_ETF']\n",
        "    TECH_STOCKS = ['Apple', 'Microsoft', 'JPMorgan']\n",
        "\n",
        "config = EnhancedTrainingConfig()\n",
        "\n",
        "# Create directories\n",
        "for directory in [config.MODEL_DIR, config.REPORTS_DIR, config.REPORTS_DIR + 'visualizations/']:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"  PPP-Q ENHANCED CLASSIFIER - PRODUCTION TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. LOAD & VALIDATE DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: LOADING & VALIDATING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "train_df = pd.read_csv(config.DATA_DIR + 'pppq_train.csv')\n",
        "val_df = pd.read_csv(config.DATA_DIR + 'pppq_val.csv')\n",
        "test_df = pd.read_csv(config.DATA_DIR + 'pppq_test.csv')\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaded:\")\n",
        "print(f\"   Train: {train_df.shape}\")\n",
        "print(f\"   Val:   {val_df.shape}\")\n",
        "print(f\"   Test:  {test_df.shape}\")\n",
        "\n",
        "# Load feature metadata\n",
        "with open(config.DATA_DIR + 'pppq_features.json', 'r') as f:\n",
        "    feature_metadata = json.load(f)\n",
        "\n",
        "print(f\"\\nüìã Features: {feature_metadata['num_features']}\")\n",
        "print(f\"   Classes: {feature_metadata['classes']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. PREPARE FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: PREPARING FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Core features\n",
        "exclude_cols = ['Date', 'Asset', 'PPP_Q_Class', 'Inflation_Regime', 'Asset_Category']\n",
        "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"\\nüìä Feature columns: {len(feature_cols)}\")\n",
        "\n",
        "# Separate features and target\n",
        "X_train = train_df[feature_cols].fillna(0)\n",
        "y_train = train_df['PPP_Q_Class']\n",
        "asset_train = train_df['Asset']\n",
        "\n",
        "X_val = val_df[feature_cols].fillna(0)\n",
        "y_val = val_df['PPP_Q_Class']\n",
        "asset_val = val_df['Asset']\n",
        "\n",
        "X_test = test_df[feature_cols].fillna(0)\n",
        "y_test = test_df['PPP_Q_Class']\n",
        "asset_test = test_df['Asset']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "class_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "\n",
        "print(f\"\\nüè∑Ô∏è  Class Mapping:\")\n",
        "for idx, class_name in class_mapping.items():\n",
        "    print(f\"   {idx}: {class_name}\")\n",
        "\n",
        "# Class distribution\n",
        "print(f\"\\nüìä Class Distribution:\")\n",
        "for split_name, y_encoded in [('TRAIN', y_train_encoded), ('VAL', y_val_encoded), ('TEST', y_test_encoded)]:\n",
        "    print(f\"\\n   {split_name}:\")\n",
        "    dist = pd.Series(y_encoded).value_counts().sort_index()\n",
        "    for idx, count in dist.items():\n",
        "        print(f\"   {class_mapping[idx]}: {count:,} ({count/len(y_encoded)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. TRAIN PRIMARY MODEL - LIGHTGBM\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: TRAINING PRIMARY MODEL - LIGHTGBM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lgb_train_data = lgb.Dataset(X_train, label=y_train_encoded, feature_name=feature_cols)\n",
        "lgb_val_data = lgb.Dataset(X_val, label=y_val_encoded, reference=lgb_train_data)\n",
        "\n",
        "print(f\"\\nüöÄ Training LightGBM...\")\n",
        "\n",
        "evals_result = {}\n",
        "callbacks = [\n",
        "    lgb.early_stopping(stopping_rounds=config.EARLY_STOPPING_ROUNDS),\n",
        "    lgb.log_evaluation(period=config.VERBOSE_EVAL),\n",
        "    lgb.record_evaluation(evals_result)\n",
        "]\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "lgbm_model = lgb.train(\n",
        "    config.LGBM_PARAMS,\n",
        "    lgb_train_data,\n",
        "    num_boost_round=config.NUM_BOOST_ROUND,\n",
        "    valid_sets=[lgb_train_data, lgb_val_data],\n",
        "    valid_names=['train', 'val'],\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "lgbm_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"\\n‚úÖ LightGBM trained!\")\n",
        "print(f\"   Best iteration: {lgbm_model.best_iteration}\")\n",
        "print(f\"   Training time: {lgbm_time:.2f}s\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. TRAIN SECONDARY MODEL - XGBOOST (Optional Ensemble)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: TRAINING SECONDARY MODEL - XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüöÄ Training XGBoost...\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Updated XGBoost API - use 'early_stopping_rounds' in __init__\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=4,\n",
        "    eval_metric='mlogloss',\n",
        "    max_depth=7,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=500,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    early_stopping_rounds=50  # Move here instead of fit()\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    X_train, y_train_encoded,\n",
        "    eval_set=[(X_val, y_val_encoded)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "xgb_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"‚úÖ XGBoost trained!\")\n",
        "print(f\"   Best iteration: {xgb_model.best_iteration}\")\n",
        "print(f\"   Training time: {xgb_time:.2f}s\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. TRAIN BASELINE - RANDOM FOREST\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: TRAINING BASELINE - RANDOM FOREST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüöÄ Training Random Forest...\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "rf_model = RandomForestClassifier(**config.RF_PARAMS)\n",
        "rf_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "rf_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"‚úÖ Random Forest trained!\")\n",
        "print(f\"   Training time: {rf_time:.2f}s\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. EVALUATE ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: EVALUATING ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def evaluate_model(model, X, y_true_encoded, model_name, model_type='lgb'):\n",
        "    \"\"\"Evaluate model with detailed metrics\"\"\"\n",
        "\n",
        "    # Predict\n",
        "    if model_type == 'lgb':\n",
        "        y_pred_proba = model.predict(X, num_iteration=model.best_iteration)\n",
        "    elif model_type == 'xgb':\n",
        "        y_pred_proba = model.predict_proba(X)\n",
        "    else:  # rf\n",
        "        y_pred_proba = model.predict_proba(X)\n",
        "\n",
        "    y_pred_encoded = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_true_encoded, y_pred_encoded)\n",
        "    balanced_acc = balanced_accuracy_score(y_true_encoded, y_pred_encoded)\n",
        "\n",
        "    unique_classes = np.unique(np.concatenate([y_true_encoded, y_pred_encoded]))\n",
        "    macro_f1 = f1_score(y_true_encoded, y_pred_encoded, average='macro', labels=unique_classes)\n",
        "    weighted_f1 = f1_score(y_true_encoded, y_pred_encoded, average='weighted', labels=unique_classes)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{model_name.upper()} - TEST SET RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    print(f\"\\nüìä Metrics:\")\n",
        "    print(f\"   Accuracy:          {accuracy:.4f}\")\n",
        "    print(f\"   Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "    print(f\"   Macro F1:          {macro_f1:.4f} ‚Üê PRIMARY METRIC\")\n",
        "    print(f\"   Weighted F1:       {weighted_f1:.4f}\")\n",
        "\n",
        "    present_classes = [label_encoder.classes_[i] for i in unique_classes]\n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true_encoded, y_pred_encoded,\n",
        "        labels=unique_classes, target_names=present_classes, zero_division=0\n",
        "    ))\n",
        "\n",
        "    cm = confusion_matrix(y_true_encoded, y_pred_encoded, labels=unique_classes)\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'predictions_encoded': y_pred_encoded,\n",
        "        'probabilities': y_pred_proba,\n",
        "        'accuracy': accuracy,\n",
        "        'balanced_accuracy': balanced_acc,\n",
        "        'macro_f1': macro_f1,\n",
        "        'weighted_f1': weighted_f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'unique_classes': unique_classes,\n",
        "        'present_classes': present_classes\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "lgbm_results = evaluate_model(lgbm_model, X_test, y_test_encoded, 'LightGBM', 'lgb')\n",
        "xgb_results = evaluate_model(xgb_model, X_test, y_test_encoded, 'XGBoost', 'xgb')\n",
        "rf_results = evaluate_model(rf_model, X_test, y_test_encoded, 'Random Forest', 'rf')\n",
        "\n",
        "# ============================================================================\n",
        "# 7. ENSEMBLE PREDICTIONS (Average of LightGBM + XGBoost)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: CREATING ENSEMBLE MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ensemble: Average probabilities\n",
        "ensemble_proba = (lgbm_results['probabilities'] + xgb_results['probabilities']) / 2\n",
        "ensemble_pred = np.argmax(ensemble_proba, axis=1)\n",
        "\n",
        "ensemble_acc = accuracy_score(y_test_encoded, ensemble_pred)\n",
        "ensemble_macro_f1 = f1_score(y_test_encoded, ensemble_pred, average='macro')\n",
        "\n",
        "print(f\"\\nüìä ENSEMBLE (LightGBM + XGBoost) RESULTS:\")\n",
        "print(f\"   Accuracy:    {ensemble_acc:.4f}\")\n",
        "print(f\"   Macro F1:    {ensemble_macro_f1:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 8: FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "importance_gain = lgbm_model.feature_importance(importance_type='gain')\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance_gain': importance_gain\n",
        "}).sort_values('importance_gain', ascending=False)\n",
        "\n",
        "print(f\"\\nüîù Top 20 Features:\")\n",
        "print(importance_df.head(20).to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 9. GENERATE ACTIONABLE INSIGHTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 9: GENERATING ACTIONABLE INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def generate_investment_signal(row, pred_class, confidence, lgbm_proba):\n",
        "    \"\"\"Generate actionable investment insights\"\"\"\n",
        "\n",
        "    asset = row['Asset']\n",
        "\n",
        "    # Determine asset category\n",
        "    if asset in config.CRYPTO_ASSETS:\n",
        "        category = 'Crypto'\n",
        "    elif asset in config.PRECIOUS_METALS:\n",
        "        category = 'Precious Metal'\n",
        "    elif asset in config.EQUITY_INDICES:\n",
        "        category = 'Equity Index'\n",
        "    elif asset in config.COMMODITIES:\n",
        "        category = 'Commodity'\n",
        "    elif asset in config.ETFS:\n",
        "        category = 'ETF'\n",
        "    else:\n",
        "        category = 'Tech Stock'\n",
        "\n",
        "    # Volatility assessment\n",
        "    volatility = row.get('Volatility_90D', 0)\n",
        "    if volatility < 15:\n",
        "        vol_level = 'LOW'\n",
        "    elif volatility < 30:\n",
        "        vol_level = 'MEDIUM'\n",
        "    elif volatility < 50:\n",
        "        vol_level = 'HIGH'\n",
        "    else:\n",
        "        vol_level = 'EXTREME'\n",
        "\n",
        "    # Cycle position\n",
        "    distance_ath = row.get('Distance_From_ATH_Pct', 0)\n",
        "    if distance_ath > -10:\n",
        "        cycle_pos = 'NEAR_ATH'\n",
        "        entry_signal = 'WAIT'\n",
        "    elif distance_ath > -30:\n",
        "        cycle_pos = 'CORRECTION'\n",
        "        entry_signal = 'WATCH'\n",
        "    elif distance_ath > -50:\n",
        "        cycle_pos = 'VALUE_ZONE'\n",
        "        entry_signal = 'CONSIDER'\n",
        "    else:\n",
        "        cycle_pos = 'DEEP_VALUE'\n",
        "        entry_signal = 'BUY'\n",
        "\n",
        "    # Growth potential\n",
        "    saturation = row.get('Market_Cap_Saturation_Pct', 50)\n",
        "    if saturation < 20:\n",
        "        growth = 'HIGH'\n",
        "    elif saturation < 50:\n",
        "        growth = 'MEDIUM'\n",
        "    elif saturation < 80:\n",
        "        growth = 'LOW'\n",
        "    else:\n",
        "        growth = 'SATURATED'\n",
        "\n",
        "    # Strengths & Weaknesses\n",
        "    strengths = []\n",
        "    weaknesses = []\n",
        "\n",
        "    # Analyze based on features\n",
        "    pp_mult = row.get('PP_Multiplier_5Y', 1.0)\n",
        "    if pp_mult > 1.5:\n",
        "        strengths.append(f\"Strong PP growth ({pp_mult:.2f}x)\")\n",
        "    elif pp_mult < 1.0:\n",
        "        weaknesses.append(f\"Losing purchasing power ({pp_mult:.2f}x)\")\n",
        "\n",
        "    sharpe = row.get('Sharpe_Ratio_5Y', 0)\n",
        "    if sharpe > 1.0:\n",
        "        strengths.append(f\"Excellent risk-adjusted returns (Sharpe: {sharpe:.2f})\")\n",
        "    elif sharpe < 0.3:\n",
        "        weaknesses.append(f\"Poor risk-adjusted returns (Sharpe: {sharpe:.2f})\")\n",
        "\n",
        "    max_dd = row.get('Max_Drawdown', 0)\n",
        "    if max_dd > 50:\n",
        "        weaknesses.append(f\"Severe drawdowns ({max_dd:.1f}%)\")\n",
        "    elif max_dd < 20:\n",
        "        strengths.append(f\"Low drawdowns ({max_dd:.1f}%)\")\n",
        "\n",
        "    if volatility < 15:\n",
        "        strengths.append(\"Low volatility (stable)\")\n",
        "    elif volatility > 40:\n",
        "        weaknesses.append(\"Extreme volatility\")\n",
        "\n",
        "    if distance_ath < -50:\n",
        "        strengths.append(\"Far from ATH (value opportunity)\")\n",
        "    elif distance_ath > -5:\n",
        "        weaknesses.append(\"Near ATH (pullback risk)\")\n",
        "\n",
        "    if growth == 'HIGH':\n",
        "        strengths.append(\"High growth potential (low saturation)\")\n",
        "    elif growth == 'SATURATED':\n",
        "        weaknesses.append(\"Limited upside (market saturated)\")\n",
        "\n",
        "    return {\n",
        "        'asset': asset,\n",
        "        'category': category,\n",
        "        'predicted_class': pred_class,\n",
        "        'confidence': round(confidence * 100, 1),\n",
        "        'volatility': vol_level,\n",
        "        'volatility_value': round(volatility, 1),\n",
        "        'cycle_position': cycle_pos,\n",
        "        'distance_from_ath': round(distance_ath, 1),\n",
        "        'entry_signal': entry_signal,\n",
        "        'growth_potential': growth,\n",
        "        'market_cap_saturation': round(saturation, 1),\n",
        "        'strengths': strengths[:3],  # Top 3\n",
        "        'weaknesses': weaknesses[:3],  # Top 3\n",
        "        'pp_multiplier_5y': round(pp_mult, 3),\n",
        "        'sharpe_ratio_5y': round(sharpe, 3),\n",
        "        'max_drawdown': round(max_dd, 1)\n",
        "    }\n",
        "\n",
        "print(\"\\nüî® Generating insights for test set...\")\n",
        "\n",
        "insights_list = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    pred_class = label_encoder.classes_[lgbm_results['predictions_encoded'][idx]]\n",
        "    confidence = lgbm_results['probabilities'][idx].max()\n",
        "\n",
        "    insights = generate_investment_signal(row, pred_class, confidence, lgbm_results['probabilities'][idx])\n",
        "    insights_list.append(insights)\n",
        "\n",
        "insights_df = pd.DataFrame(insights_list)\n",
        "\n",
        "# Save insights\n",
        "insights_path = config.REPORTS_DIR + 'investment_insights.csv'\n",
        "insights_df.to_csv(insights_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(insights_df)} actionable insights\")\n",
        "print(f\"\\nüìã Sample Insights (Bitcoin):\")\n",
        "bitcoin_insights = insights_df[insights_df['asset'] == 'Bitcoin'].head(5)\n",
        "for col in ['asset', 'predicted_class', 'confidence', 'volatility', 'entry_signal', 'cycle_position']:\n",
        "    if col in bitcoin_insights.columns:\n",
        "        print(f\"   {col}: {bitcoin_insights.iloc[0][col]}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 10. SAVE MODELS & ARTIFACTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 10: SAVING MODELS & ARTIFACTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save models\n",
        "lgbm_model.save_model(config.MODEL_DIR + 'lgbm_model.txt')\n",
        "xgb_model.save_model(config.MODEL_DIR + 'xgb_model.json')\n",
        "\n",
        "with open(config.MODEL_DIR + 'rf_model.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "with open(config.MODEL_DIR + 'label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "with open(config.MODEL_DIR + 'feature_columns.json', 'w') as f:\n",
        "    json.dump(feature_cols, f, indent=2)\n",
        "\n",
        "importance_df.to_csv(config.REPORTS_DIR + 'feature_importance.csv', index=False)\n",
        "\n",
        "# Save comprehensive results\n",
        "results_summary = {\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'models': {\n",
        "        'lightgbm': {\n",
        "            'macro_f1': float(lgbm_results['macro_f1']),\n",
        "            'accuracy': float(lgbm_results['accuracy']),\n",
        "            'best_iteration': int(lgbm_model.best_iteration),\n",
        "            'training_time_seconds': lgbm_time\n",
        "        },\n",
        "        'xgboost': {\n",
        "            'macro_f1': float(xgb_results['macro_f1']),\n",
        "            'accuracy': float(xgb_results['accuracy']),\n",
        "            'best_iteration': int(xgb_model.best_iteration),\n",
        "            'training_time_seconds': xgb_time\n",
        "        },\n",
        "        'random_forest': {\n",
        "            'macro_f1': float(rf_results['macro_f1']),\n",
        "            'accuracy': float(rf_results['accuracy']),\n",
        "            'training_time_seconds': rf_time\n",
        "        },\n",
        "        'ensemble': {\n",
        "            'macro_f1': float(ensemble_macro_f1),\n",
        "            'accuracy': float(ensemble_acc)\n",
        "        }\n",
        "    },\n",
        "    'best_model': 'LightGBM' if lgbm_results['macro_f1'] >= xgb_results['macro_f1'] else 'XGBoost',\n",
        "    'top_10_features': importance_df.head(10)['feature'].tolist(),\n",
        "    'num_features': len(feature_cols),\n",
        "    'classes': label_encoder.classes_.tolist()\n",
        "}\n",
        "\n",
        "with open(config.REPORTS_DIR + 'training_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ All models and artifacts saved!\")\n",
        "\n",
        "# ============================================================================\n",
        "# 11. FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä MODEL PERFORMANCE COMPARISON (Test Set):\")\n",
        "print(f\"   {'Model':<20} {'Macro F1':<12} {'Accuracy':<12}\")\n",
        "print(f\"   {'-'*44}\")\n",
        "print(f\"   {'LightGBM':<20} {lgbm_results['macro_f1']:<12.4f} {lgbm_results['accuracy']:<12.4f}\")\n",
        "print(f\"   {'XGBoost':<20} {xgb_results['macro_f1']:<12.4f} {xgb_results['accuracy']:<12.4f}\")\n",
        "print(f\"   {'Random Forest':<20} {rf_results['macro_f1']:<12.4f} {rf_results['accuracy']:<12.4f}\")\n",
        "print(f\"   {'Ensemble':<20} {ensemble_macro_f1:<12.4f} {ensemble_acc:<12.4f}\")\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {results_summary['best_model']}\")\n",
        "\n",
        "print(f\"\\nüìÅ OUTPUT FILES:\")\n",
        "print(f\"   Models: {config.MODEL_DIR}\")\n",
        "print(f\"   Reports: {config.REPORTS_DIR}\")\n",
        "print(f\"   Insights: {insights_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ]
}